{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "- 텐서는 배열이나 행렬과 유사한 자료구조이다.\n",
    "- Pytorch에서는 텐서를 사용하여 모델의 입력(input)과 출력(output), 모델의 매개변수를 부호화(encode)한다.\n",
    "- GPU를 이용하여 가속화하여 연산할 수 있다.\n",
    "- 자동 미분에 최적화 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:1.13.1+cu117\n",
      "np:1.26.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"torch:{torch.__version__}\")\n",
    "print(f\"np:{np.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서의 초기화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "## 데이터로 부터 직접 만들기\n",
    "data = [    [1,2],\n",
    "            [3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "## 데이터의 자료형은 자동으로 유추한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "## numpy로 부터 초기화\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.9024, 0.1748],\n",
      "        [0.5865, 0.1016]])\n"
     ]
    }
   ],
   "source": [
    "## 다른 tensor로 부터 생성\n",
    "x_ones = torch.ones_like(x_data) # x_data의 속성(차원정보/data type(dtype)을 유지함\n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float32) #rand의 경우 x_data의 차원정보만 덮어씀\n",
    "print(x_rand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 무작위나 상수값 쓰기\n",
    "- shape은 텐서의 차원정보를 나타내며 자료형은 튜플. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3338, 0.2309, 0.1046],\n",
      "        [0.1566, 0.3407, 0.2690]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "shape = (2,3,) # 3개 요소를 가진 총 2개의 정보\n",
    "\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"{rand_tensor}\\n{ones_tensor}\\n{zeros_tensor}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서의 속성 (Attribute)\n",
    "- 텐서의 속성은 1. 텐서의 모양, 2. 자료형 및 어느장치에 저장되는지를 나타낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((3,4))\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서의 연산\n",
    "- 전치, 인덱싱, 슬라이싱, 수학 계산, 선형 대수, 임의 샘플링 등 연산을 수행할 수 있다.\n",
    "- GPU에서도 가능함.\n",
    "- 기본적 텐서는 cpu에 생성되며 .to 메서드를 사용 시 GPU로 텐서를 명시적 이동 가능\n",
    "- 장치들 간 큰 텐서의 값을 이동 시키는 것은 공간,시간적 비용이 크다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 존재하면 텐서를 이동합니다\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1row = tensor([1., 1., 1., 1.])\n",
      "1col = tensor([1., 1., 1., 1.])\n",
      "last col = tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Numpy식 텐서 연산\n",
    "\n",
    "tensor = torch.ones(4,4)\n",
    "\n",
    "print(f\"1row = {tensor[0]}\") # 0행 출력\n",
    "print(f\"1col = {tensor[:,0]}\") # 0열 출력\n",
    "print(f\"last col = {tensor[..., -1]}\") # 마지막열(-1번째열) 출력\n",
    "tensor[:,1] = 0 # 1열 0으로 초기화\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n",
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서합치기\n",
    "# 위 아래로 합치기. dim = 0, vstack((x,y)), row_stack\n",
    "t_down = torch.cat([tensor, tensor, tensor], dim = 0)\n",
    "print(t_down)\n",
    "\n",
    "# 옆으로 합치기. dim = 1, hstack((x,y)), column_stack\n",
    "t_next = torch.cat([tensor, tensor, tensor], dim = 1)\n",
    "print(t_next)\n",
    "\n",
    "# 새로운 차원 추가. dim = 0은 두개의 텐서를 위아래로 차원 추가. dim = 1은 새로운 텐서를 추가해서 좌우로 합쳐줌\n",
    "t_stack_0 = torch.stack([tensor, tensor], dim = 0)\n",
    "print(t_stack_0)\n",
    "t_stack_1 = torch.stack([tensor, tensor], dim = 1)\n",
    "print(t_stack_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "## 산술 연산\n",
    "## 두 텐서간의 행렬 곱(matrix multiplication)을 계산합니다.\n",
    "## y1,y2,y3은 다 같은값이다.\n",
    "## .T는 전치행렬으로 가져온다.\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "## 요소곱 (element - wise product, z1=z2=z3)\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out = z3)\n",
    "\n",
    "print(y1, y2, y3, z1, z2 ,z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.) 12.0 <class 'torch.Tensor'> <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "## 단일요소텐서\n",
    "## 텐서의 모든 값을 하나로 단일 집계하여 요소가 하나인 텐서의 경우, item()을 사용하여 python 숫자값으로 변환 가능함\n",
    "\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "\n",
    "print(\n",
    "    tensor.sum(),\n",
    "    tensor.sum().item(),\n",
    "    type(tensor.sum()),\n",
    "    type(tensor.sum().item())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "## 바꿔치기 연산\n",
    "## 연산 결과를 피연산자에 저장하는 연산. _접미사가 붙음\n",
    "## x.copy_(y)나 x._t()는 x를 변경함\n",
    "\n",
    "print(f\"{tensor}\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m5\u001b[39m)\n\u001b[0;32m      5\u001b[0m t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m n \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(t,n)\n\u001b[0;32m     10\u001b[0m t\u001b[39m.\u001b[39madd_(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "## Numpy변환\n",
    "## cPU 상의 텐서와 NumPy 배열은 메모리 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경됩니다.\n",
    "## 그 반대의 경우에도 변경 사항이 공유됨\n",
    "t = torch.ones(5)\n",
    "t = t.to(\"cuda\")\n",
    "n = t.numpy()\n",
    "\n",
    "print(t,n)\n",
    "\n",
    "t.add_(1)\n",
    "print(t,n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k_public",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
